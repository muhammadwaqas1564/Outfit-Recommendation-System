{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18aaeb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\ayesha\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\ayesha\\anaconda3\\lib\\site-packages (1.24.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ayesha\\anaconda3\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: torch in c:\\users\\ayesha\\anaconda3\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\ayesha\\anaconda3\\lib\\site-packages (0.19.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\ayesha\\anaconda3\\lib\\site-packages (9.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\ayesha\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ayesha\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\ayesha\\anaconda3\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\ayesha\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\ayesha\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ayesha\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\ayesha\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\ayesha\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\ayesha\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ayesha\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ayesha\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ayesha\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ayesha\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\ayesha\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "!pip install pandas numpy scikit-learn torch torchvision pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11ef26c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file loaded successfully.\n",
      "      id gender masterCategory subCategory  articleType baseColour  season  \\\n",
      "0  15970    Men        Apparel     Topwear       Shirts  Navy Blue    Fall   \n",
      "1  39386    Men        Apparel  Bottomwear        Jeans       Blue  Summer   \n",
      "2  59263  Women    Accessories     Watches      Watches     Silver  Winter   \n",
      "3  21379    Men        Apparel  Bottomwear  Track Pants      Black    Fall   \n",
      "4  53759    Men        Apparel     Topwear      Tshirts       Grey  Summer   \n",
      "\n",
      "     year   usage                             productDisplayName  \n",
      "0  2011.0  Casual               Turtle Check Men Navy Blue Shirt  \n",
      "1  2012.0  Casual             Peter England Men Party Blue Jeans  \n",
      "2  2016.0  Casual                       Titan Women Silver Watch  \n",
      "3  2011.0  Casual  Manchester United Men Solid Black Track Pants  \n",
      "4  2012.0  Casual                          Puma Men Grey T-shirt  \n",
      "Index(['id', 'gender', 'masterCategory', 'subCategory', 'articleType',\n",
      "       'baseColour', 'season', 'year', 'usage', 'productDisplayName'],\n",
      "      dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 44424 entries, 0 to 44423\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   id                  44424 non-null  int64  \n",
      " 1   gender              44424 non-null  object \n",
      " 2   masterCategory      44424 non-null  object \n",
      " 3   subCategory         44424 non-null  object \n",
      " 4   articleType         44424 non-null  object \n",
      " 5   baseColour          44409 non-null  object \n",
      " 6   season              44403 non-null  object \n",
      " 7   year                44423 non-null  float64\n",
      " 8   usage               44107 non-null  object \n",
      " 9   productDisplayName  44417 non-null  object \n",
      "dtypes: float64(1), int64(1), object(8)\n",
      "memory usage: 3.4+ MB\n",
      "None\n",
      "id                      0\n",
      "gender                  0\n",
      "masterCategory          0\n",
      "subCategory             0\n",
      "articleType             0\n",
      "baseColour             15\n",
      "season                 21\n",
      "year                    1\n",
      "usage                 317\n",
      "productDisplayName      7\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define file paths\n",
    "csv_file_path = r'C:\\Users\\Ayesha\\Documents\\Internship Projects\\ITSOLERA\\project\\styles.csv'\n",
    "image_folder = r'C:\\Users\\Ayesha\\Documents\\Internship Projects\\ITSOLERA\\project\\images'\n",
    "\n",
    "# Check if file exists and load data\n",
    "if os.path.exists(csv_file_path):\n",
    "    try:\n",
    "        styles_df = pd.read_csv(\n",
    "            csv_file_path,\n",
    "            delimiter=',',\n",
    "            encoding='utf-8',\n",
    "            on_bad_lines='skip',\n",
    "            quotechar='\"',\n",
    "            escapechar='\\\\'\n",
    "        )\n",
    "        print(\"CSV file loaded successfully.\")\n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f\"ParserError: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "else:\n",
    "    print(\"The file does not exist.\")\n",
    "\n",
    "# Display the first few rows and summary\n",
    "print(styles_df.head())\n",
    "print(styles_df.columns)\n",
    "print(styles_df.info())\n",
    "\n",
    "# Check for missing values\n",
    "print(styles_df.isnull().sum())\n",
    "\n",
    "# Drop or fill missing values\n",
    "styles_df = styles_df.dropna(subset=['masterCategory', 'subCategory'])\n",
    "\n",
    "# Encode categorical columns\n",
    "label_encoder = LabelEncoder()\n",
    "styles_df['masterCategory_encoded'] = label_encoder.fit_transform(styles_df['masterCategory'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "067420f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define image preprocessing function\n",
    "def preprocess_image(image_path, target_size=(128, 128)):\n",
    "    \"\"\"Load an image file and preprocess it.\"\"\"\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(target_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return preprocess(image)\n",
    "\n",
    "# Create a custom Dataset class\n",
    "class ClothingDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, f'{row[\"id\"]}.jpg')\n",
    "        image = preprocess_image(image_path) if os.path.exists(image_path) else torch.zeros(3, 128, 128)\n",
    "        label = row[\"masterCategory_encoded\"]\n",
    "        return image, label\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_df, val_df = train_test_split(styles_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = ClothingDataset(train_df, image_folder)\n",
    "val_dataset = ClothingDataset(val_df, image_folder)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79eec97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the CNN model\n",
    "class ClothingClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ClothingClassifier, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 16 * 16, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "num_classes = len(styles_df['masterCategory_encoded'].unique())\n",
    "model = ClothingClassifier(num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3f33665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.1777\n",
      "Validation Accuracy: 0.9778\n",
      "Epoch 2/10, Loss: 0.0899\n",
      "Validation Accuracy: 0.9788\n",
      "Epoch 3/10, Loss: 0.0601\n",
      "Validation Accuracy: 0.9835\n",
      "Epoch 4/10, Loss: 0.0498\n",
      "Validation Accuracy: 0.9857\n",
      "Epoch 5/10, Loss: 0.0376\n",
      "Validation Accuracy: 0.9836\n",
      "Epoch 6/10, Loss: 0.0264\n",
      "Validation Accuracy: 0.9849\n",
      "Epoch 7/10, Loss: 0.0260\n",
      "Validation Accuracy: 0.9865\n",
      "Epoch 8/10, Loss: 0.0208\n",
      "Validation Accuracy: 0.9847\n",
      "Epoch 9/10, Loss: 0.0196\n",
      "Validation Accuracy: 0.9795\n",
      "Epoch 10/10, Loss: 0.0245\n",
      "Validation Accuracy: 0.9849\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device).long()  # Convert labels to torch.long\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    corrects = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device).long()  # Convert labels to torch.long\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            corrects += torch.sum(preds == labels.data).item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    val_accuracy = corrects / total\n",
    "    print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "    \n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'fashion_outfit_recommendation_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2861148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the CNN model architecture again\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 16 * 16)  # Flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e2f0659",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayesha\\AppData\\Local\\Temp\\ipykernel_12492\\3321150632.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load('fashion_outfit_recommendation_model.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU()\n",
       "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (4): ReLU()\n",
       "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (7): ReLU()\n",
       "  (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (9): Flatten(start_dim=1, end_dim=-1)\n",
       "  (10): Linear(in_features=32768, out_features=512, bias=True)\n",
       "  (11): ReLU()\n",
       "  (12): Dropout(p=0.5, inplace=False)\n",
       "  (13): Linear(in_features=512, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the state dict\n",
    "state_dict = torch.load('fashion_outfit_recommendation_model.pth')\n",
    "\n",
    "# Remove 'model.' prefix from the keys\n",
    "new_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    name = k.replace('model.', '')  # Remove 'model.' prefix\n",
    "    new_state_dict[name] = v\n",
    "\n",
    "# Load the modified state dict\n",
    "model.load_state_dict(new_state_dict)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b254649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayesha\\AppData\\Local\\Temp\\ipykernel_12492\\951507578.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load('fashion_outfit_recommendation_model.pth')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Correct model architecture\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    \n",
    "    nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    \n",
    "    nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    \n",
    "    nn.Flatten(),\n",
    "    \n",
    "    # Adjust the input size here\n",
    "    nn.Linear(128 * 16 * 16, 512),  # 128 * 16 * 16 = 32768\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    \n",
    "    nn.Linear(512, len(styles_df['masterCategory_encoded'].unique()))  # Output layer\n",
    ")\n",
    "\n",
    "# Load the saved state dict\n",
    "state_dict = torch.load('fashion_outfit_recommendation_model.pth')\n",
    "\n",
    "# Remove 'model.' prefix from the keys in the state dictionary\n",
    "new_state_dict = {k.replace('model.', ''): v for k, v in state_dict.items()}\n",
    "\n",
    "# Load the updated state dictionary into the model\n",
    "model.load_state_dict(new_state_dict)\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78c0159d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully without warnings.\n"
     ]
    }
   ],
   "source": [
    "# Load the saved state dict with weights_only=True\n",
    "state_dict = torch.load('fashion_outfit_recommendation_model.pth', weights_only=True)\n",
    "\n",
    "# Remove 'model.' prefix from the keys in the state dictionary\n",
    "new_state_dict = {k.replace('model.', ''): v for k, v in state_dict.items()}\n",
    "\n",
    "# Load the updated state dictionary into the model\n",
    "model.load_state_dict(new_state_dict)\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully without warnings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "485d8760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6640a028",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutfitDataset(Dataset):\n",
    "    def __init__(self, image_data, metadata, labels, transform=None):\n",
    "        self.image_data = image_data  # List of images (file paths or tensors)\n",
    "        self.metadata = metadata      # Tabular data for each image\n",
    "        self.labels = labels          # Outfit compatibility labels (1 if compatible, 0 otherwise)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.image_data[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        metadata = self.metadata[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return image, metadata, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d69a19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageFeatureExtractor, self).__init__()\n",
    "        # Use a pre-trained ResNet model\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        # Remove the final layer\n",
    "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a619ec4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetadataProcessor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128):\n",
    "        super(MetadataProcessor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6fca7303",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutfitRecommendationModel(nn.Module):\n",
    "    def __init__(self, metadata_input_size, hidden_size=128):\n",
    "        super(OutfitRecommendationModel, self).__init__()\n",
    "        # Image feature extractor\n",
    "        self.image_extractor = ImageFeatureExtractor()\n",
    "\n",
    "        # Metadata processor\n",
    "        self.metadata_processor = MetadataProcessor(metadata_input_size, hidden_size)\n",
    "\n",
    "        # Final layers combining both image and metadata features\n",
    "        combined_input_size = 2048 + hidden_size  # ResNet outputs 2048-dim vector\n",
    "        self.fc1 = nn.Linear(combined_input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)  # Output compatibility score (1 for compatible, 0 for not)\n",
    "\n",
    "    def forward(self, image, metadata):\n",
    "        # Process image features\n",
    "        image_features = self.image_extractor(image)\n",
    "\n",
    "        # Process metadata features\n",
    "        metadata_features = self.metadata_processor(metadata)\n",
    "\n",
    "        # Concatenate image and metadata features\n",
    "        combined_features = torch.cat((image_features, metadata_features), dim=1)\n",
    "\n",
    "        # Final prediction layers\n",
    "        x = F.relu(self.fc1(combined_features))\n",
    "        x = torch.sigmoid(self.fc2(x))  # Output a score between 0 and 1\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ee15d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, num_epochs=10, learning_rate=0.001):\n",
    "    criterion = nn.BCELoss()  # Binary Cross Entropy Loss for compatibility\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, metadata, labels in dataloader:\n",
    "            # Forward pass\n",
    "            outputs = model(images, metadata)\n",
    "            loss = criterion(outputs.squeeze(), labels.float())\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader)}')\n",
    "\n",
    "    print('Training completed!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e0370a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "def preprocess_metadata(metadata_df):\n",
    "    # Assuming metadata_df is a pandas DataFrame containing the columns articleType, baseColour, etc.\n",
    "    label_encoders = {}\n",
    "    encoded_metadata = np.zeros_like(metadata_df)\n",
    "\n",
    "    for col in metadata_df.columns:\n",
    "        le = LabelEncoder()\n",
    "        encoded_metadata[col] = le.fit_transform(metadata_df[col])\n",
    "        label_encoders[col] = le\n",
    "\n",
    "    return encoded_metadata, label_encoders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d29af55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayesha\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ayesha\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\Ayesha/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [01:44<00:00, 981kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.8879333883523941\n",
      "Epoch [2/5], Loss: 0.48354943096637726\n",
      "Epoch [3/5], Loss: 0.17327819392085075\n",
      "Epoch [4/5], Loss: 0.2069761073216796\n",
      "Epoch [5/5], Loss: 0.7092091618105769\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Example dummy data\n",
    "image_data = torch.randn(100, 3, 224, 224)  # 100 random image tensors\n",
    "metadata = torch.randn(100, 5)  # 5 tabular features (articleType, baseColour, etc.)\n",
    "labels = torch.randint(0, 2, (100,))  # Random compatibility labels (0 or 1)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = OutfitDataset(image_data, metadata, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize and train the model\n",
    "metadata_input_size = metadata.shape[1]\n",
    "model = OutfitRecommendationModel(metadata_input_size)\n",
    "train_model(model, dataloader, num_epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d0190274",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'fashion_outfit_recommendation_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8acf2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
